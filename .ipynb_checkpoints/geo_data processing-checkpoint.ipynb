{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for geo data processing\n",
    "geopandas has conflicts with anaconda, so install and use it in a virtual env called geoenv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely import wkt\n",
    "from shapely.geometry import *\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruoying\\anaconda3\\envs\\geoenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (10,12,13,20,38,45,46,55,59) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "fire_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/Fire_Incidents_SF.csv'\n",
    "df_fire = pd.read_csv(fire_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Incident Number', 'Exposure Number', 'ID', 'Address', 'Incident Date',\n",
       "       'Call Number', 'Alarm DtTm', 'Arrival DtTm', 'Close DtTm', 'City',\n",
       "       'zipcode', 'Battalion', 'Station Area', 'Box', 'Suppression Units',\n",
       "       'Suppression Personnel', 'EMS Units', 'EMS Personnel', 'Other Units',\n",
       "       'Other Personnel', 'First Unit On Scene', 'Estimated Property Loss',\n",
       "       'Estimated Contents Loss', 'Fire Fatalities', 'Fire Injuries',\n",
       "       'Civilian Fatalities', 'Civilian Injuries', 'Number of Alarms',\n",
       "       'Primary Situation', 'Mutual Aid', 'Action Taken Primary',\n",
       "       'Action Taken Secondary', 'Action Taken Other',\n",
       "       'Detector Alerted Occupants', 'Property Use', 'Area of Fire Origin',\n",
       "       'Ignition Cause', 'Ignition Factor Primary',\n",
       "       'Ignition Factor Secondary', 'Heat Source', 'Item First Ignited',\n",
       "       'Human Factors Associated with Ignition', 'Structure Type',\n",
       "       'Structure Status', 'Floor of Fire Origin', 'Fire Spread',\n",
       "       'No Flame Spead', 'Number of floors with minimum damage',\n",
       "       'Number of floors with significant damage',\n",
       "       'Number of floors with heavy damage',\n",
       "       'Number of floors with extreme damage', 'Detectors Present',\n",
       "       'Detector Type', 'Detector Operation', 'Detector Effectiveness',\n",
       "       'Detector Failure Reason', 'Automatic Extinguishing System Present',\n",
       "       'Automatic Extinguishing Sytem Type',\n",
       "       'Automatic Extinguishing Sytem Perfomance',\n",
       "       'Automatic Extinguishing Sytem Failure Reason',\n",
       "       'Number of Sprinkler Heads Operating', 'Supervisor District',\n",
       "       'neighborhood_district', 'point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fire data\n",
    "df_fire.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the primary situation: causes for fire. No null value here. very complete data\n",
    "df_fire['Primary Situation'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only focus on fire that are due to leakage, accidents, operation, etc.\n",
    "\n",
    "412 - Gas leak (natural gas or LPG)\n",
    "210 - Steam Rupture, steam, other\n",
    "522 - Water or steam leak\n",
    "520 - Water problem, other\n",
    "113 - Cooking fire, confined to container\n",
    "411 - Gasoline or other flammable liquid spill\n",
    "413 - Oil or other combustible liquid spill\n",
    "444 - Power line down (wire)\n",
    "653 - Barbecue, tar kettle'\n",
    "445 - Arcing, shorted electrical equipment\n",
    "730 - System malfunction, other\n",
    "220 - Rupture from air or gas, other\n",
    "223 - Air/gas rupture of pressure or process\n",
    "114 - Chimney/flue fire, confined to chimney\n",
    "410 - Flammable gas or liquid condition, other\n",
    "440 - Elec. wiring/equip. problem, other\n",
    "652 - Steam/vapor/fog/dust mistaken for smoke\n",
    "221 - Rupture air or gas, pipe/pipline\n",
    "521 - Water evacuation\n",
    "650 - Steam/gas mistaken for smoke, other\n",
    "422 - Chemical spill or leak(haz.)\n",
    "213 - Steam rupture, pressure/process vessel\n",
    "671 - Hazmat release investigation w/no hazmat\n",
    "200 - Overpressure rupture/explosion, overheat\n",
    "441 - Heat- short circuit, defect/worn wiring\n",
    "443 - Light ballast breakdown\n",
    "423 - Refrigeration leak\n",
    "420 - Toxic condition, other (haz.)\n",
    "431 - Radiation leak, radioactive material\n",
    "251 - Excess heat, scorth burns with no ign\n",
    "371 - Electrocution or potential electrocution\n",
    "212 - Overpressure rupture of steam boiler\n",
    "222 - Rupture of boiler from air or gas\n",
    "211 - Steam Rupture, Pipe or pipeline\n",
    "231 - Chem. reaction rupture of process vessel\n",
    "115 - Incinerator overload/mal.,fire confined\n",
    "424 - Carbon monoxide incident21\n",
    "430 - Radioactive condition, other\n",
    "440 Electrical  wiring/equipment problem, other\n",
    "117 Commercial Compactor fire, confined to rubbish\n",
    "411 Gasoline or other flammable liquid spill\n",
    "444 Power line down\n",
    "531 Smoke or odor removal\n",
    "412 Gas leak (natural gas or LPG)\n",
    "210 Overpressure rupture from steam, other\n",
    "113 Cooking fire, confined to container\n",
    "651 Smoke scare, odor of smoke\n",
    "653 Smoke from barbecue, tar kettle\n",
    "445 Arcing, shorted electrical equipment\n",
    "441 Heat from short circuit (wiring), defective/worn\n",
    "423 Refrigeration leak\n",
    "222 Overpressure rupture of boiler from air or gas\n",
    "410 Combustible/flammable gas/liquid condition, other\n",
    "424 Carbon monoxide incident\n",
    "200 Overpressure rupture, explosion, overheat other\n",
    "413 Oil or other combustible liquid spill\n",
    "114 Chimney or flue fire, confined to chimney or flue\n",
    "420 Toxic condition, other\n",
    "220 Overpressure rupture from air or gas, other\n",
    "451 Biological hazard, confirmed or suspected\n",
    "443 Breakdown of light ballast\n",
    "213 Steam rupture of pressure or process vessel\n",
    "163 Outside gas or vapor combustion explosion\n",
    "116 Fuel burner/boiler malfunction, fire confined\n",
    "231 Chemical reaction rupture of process vessel\n",
    "431 Radiation leak, radioactive material\n",
    "430 Radioactive condition, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for internal fire.\n",
    "internal_fire = ['412','210','522','520','113','411','444','653','445','730',\\\n",
    "                 '220','223','114','410','440','652','221','521','650','422',\\\n",
    "                 '213','671','200','441','443','423','420','431','251','371',\\\n",
    "                 '212','222','211','231','115','424','430','440','117','411',\\\n",
    "                 '531','412','210','651','445','200','413','114','451','443',\\\n",
    "                 '163','116','231','431']\n",
    "df_internal_fire = df_fire[df_fire['Primary Situation'].str.contains('|'.join(internal_fire))]\n",
    "df_internal_fire.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write function to process data\n",
    "data assembly:\n",
    "id, year, crime, occupancy, demographic, inspection, violation, fire incident last year, fire incident this year\n",
    "\n",
    "df_parcel: gpd;\n",
    "\n",
    "other df: pandas df\n",
    "\n",
    "year: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parcel_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_parcel_lu/sf_parcel.geojson'\n",
    "#df_parcel = gpd.read_file(parcel_file)\n",
    "    \n",
    "def process_fire(df_parcel, df_fire, year):\n",
    "    \n",
    "    # in order to spatial join, only need id and geometry to keep it simple\n",
    "    df_parcel_simple = df_parcel[['objectid', 'geometry']]\n",
    "    \n",
    "    # create a buffer\n",
    "    gdf_parcel_buffer = df_parcel_simple\n",
    "    gdf_parcel_buffer.geometry = df_parcel_simple.geometry.buffer(0.0001) # unit is degree: 0.0003 is about 33 m or 100 feet\n",
    "    \n",
    "    # see if any of the incidents features is useful\n",
    "    # if we are only counting incidents, then only incidents number,date and point are useful\n",
    "    df_fire = df_fire[['Incident Number','Incident Date','point']]\n",
    "    \n",
    "    # select the year and previous year\n",
    "    df_fire['Incident Date'] = pd.to_datetime(df_fire['Incident Date'])\n",
    "    df_fire['year'] = df_fire['Incident Date'].dt.year\n",
    "    # select this year and previous year\n",
    "    df_fire = df_fire[(df_fire['year'] == year)|(df_fire['year'] == year-1)]\n",
    "    \n",
    "    df_fire['point'] = df_fire['point'].astype(str)\n",
    "    df_fire = df_fire.dropna() # axis = 'point'\n",
    "\n",
    "    # it seems that there is no null value, but instead, have a \"nan\" as a string\n",
    "    df_fire = df_fire[df_fire['point'] != 'nan']\n",
    "    df_fire['point'] = df_fire['point'].apply(wkt.loads)\n",
    "    \n",
    "    # change the fire df into geo df\n",
    "    gdf_fire = gpd.GeoDataFrame(df_fire, geometry='point')\n",
    "\n",
    "    # make sure the two dataset have the same projection\n",
    "    gdf_fire.crs = df_parcel_simple.crs\n",
    "\n",
    "    # divide the data into current year and previous year\n",
    "    gdf_fire_current_year = gdf_fire[(gdf_fire['year'] == year)]\n",
    "    gdf_fire_previous_year = gdf_fire[(gdf_fire['year'] == year-1)]\n",
    "    \n",
    "    ########################## current year\n",
    "    # spatial join: using buffer\n",
    "    fire_sjoin_current_year = gpd.sjoin(gdf_fire_current_year,gdf_parcel_buffer,how = 'left', op='within')\n",
    "\n",
    "    # drop na\n",
    "    fire_sjoin_current_year = fire_sjoin_current_year.dropna()\n",
    "    # group by parcel id\n",
    "    grouped_current_year = fire_sjoin_current_year.groupby('objectid').size()\n",
    "    df_group_current_year = grouped_current_year.to_frame().reset_index()\n",
    "    df_group_current_year.columns = ['objectid','fire_count']\n",
    "    \n",
    "    ###################### previous year\n",
    "    # spatial join: using buffer\n",
    "    fire_sjoin_previous_year = gpd.sjoin(gdf_fire_previous_year,gdf_parcel_buffer,how = 'left', op='within')\n",
    "\n",
    "    # drop na\n",
    "    fire_sjoin_previous_year = fire_sjoin_previous_year.dropna()\n",
    "    # group by parcel id\n",
    "    grouped_previous_year = fire_sjoin_previous_year.groupby('objectid').size()\n",
    "    df_group_previous_year = grouped_previous_year.to_frame().reset_index()\n",
    "    df_group_previous_year.columns = ['objectid','fire_count_last year']\n",
    "    \n",
    "    ######### join both\n",
    "    # first convert gpd to pd\n",
    "    parcel = pd.DataFrame(df_parcel.drop(columns='geometry'))\n",
    "\n",
    "    # then merge using objectid\n",
    "    merged_fire_data = parcel.merge(df_group_current_year, on='objectid', how='outer')\n",
    "    merged_fire_data = merged_fire_data.merge(df_group_previous_year, on='objectid', how='outer')\n",
    "    \n",
    "    # replace nan with 0\n",
    "    merged_fire_data['fire_count'] = merged_fire_data['fire_count'].fillna(0)\n",
    "    merged_fire_data['fire_count_last year'] = merged_fire_data['fire_count_last year'].fillna(0)\n",
    "\n",
    "    merged_fire_data['year'] = year \n",
    "    \n",
    "    return merged_fire_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process inspection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the types of inspection before inputing this function\n",
    "def process_inspection(df_parcel, df_inspect, year):\n",
    "    \n",
    "    # in order to spatial join, only need id and geometry to keep it simple\n",
    "    df_parcel_simple = df_parcel[['objectid', 'geometry']]\n",
    "    \n",
    "    # select \n",
    "    \n",
    "    df_inspect['Inspection Start Date'] = pd.to_datetime(df_inspect['Inspection Start Date'])\n",
    "    df_inspect['year'] = df_inspect['Inspection Start Date'].dt.year\n",
    "    df_inspect = df_inspect[(df_inspect['year'] == year)]\n",
    "    \n",
    "    # this need to drop na in geometry fileds, otherwise it would fail\n",
    "    df_inspect = df_inspect.dropna(subset=['point'])\n",
    "    df_inspect['point'] = df_inspect.apply(lambda row: shape(json.loads(row['point'].replace(\"\\'\", \"\\\"\"))), axis=1)\n",
    "    gdf_inspect = gpd.GeoDataFrame(df_inspect, geometry='point')\n",
    "\n",
    "    # make sure the two dataset have the same projection\n",
    "    gdf_inspect.crs = df_parcel_simple.crs\n",
    "    \n",
    "    inspect_sjoin = gpd.sjoin(gdf_inspect, df_parcel_simple, how=\"inner\", op='within')\n",
    "    \n",
    "    # dropnas\n",
    "    inspect_sjoin = inspect_sjoin.dropna()\n",
    "    \n",
    "    # group by parcel id\n",
    "    grouped = inspect_sjoin.groupby('objectid').size()\n",
    "    df_group = grouped.to_frame().reset_index()\n",
    "    df_group.columns = ['objectid','complaints_count']\n",
    "    \n",
    "    ##### join with parcel\n",
    "    # first convert gpd to pd\n",
    "    #parcel = pd.DataFrame(df_parcel.drop(columns='geometry'))\n",
    "\n",
    "    # then merge using objectid\n",
    "    #merged_inspect_data = parcel.merge(df_group, on='objectid', how='outer')\n",
    "    \n",
    "    # replace nan with 0\n",
    "    #merged_inspect_data['complaints_count'] = merged_inspect_data['complaints_count'].fillna(0)\n",
    "\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process violation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_violation(df_parcel, df_violate, year):\n",
    "    \n",
    "    # in order to spatial join, only need id and geometry to keep it simple\n",
    "    df_parcel_simple = df_parcel[['objectid', 'geometry']]\n",
    "    \n",
    "    # select year \n",
    "    df_violate['violation date'] = pd.to_datetime(df_violate['violation date'])\n",
    "    df_violate['year'] = df_violate['violation date'].dt.year\n",
    "    df_violate = df_violate[(df_violate['year'] == year)]\n",
    "    \n",
    "    # this need to drop na in geometry fileds, otherwise it would fail\n",
    "    df_violate = df_violate.dropna(subset=['Location'])\n",
    "    df_violate['Location'] = df_violate.apply(lambda row: shape(json.loads(row['Location'].replace(\"\\'\", \"\\\"\"))), axis=1)\n",
    "    gdf_violate = gpd.GeoDataFrame(df_violate, geometry='Location')\n",
    "\n",
    "    # make sure the two dataset have the same projection\n",
    "    gdf_violate.crs = df_parcel_simple.crs\n",
    "    \n",
    "    violate_sjoin = gpd.sjoin(gdf_violate, df_parcel_simple, how=\"inner\", op='within')\n",
    "    \n",
    "    # dropnas\n",
    "    violate_sjoin = violate_sjoin.dropna()\n",
    "    \n",
    "    # group by parcel id\n",
    "    grouped = violate_sjoin.groupby('objectid').size()\n",
    "    df_group = grouped.to_frame().reset_index()\n",
    "    df_group.columns = ['objectid','violation_count']\n",
    "    \n",
    "    ##### join with parcel\n",
    "    # first convert gpd to pd\n",
    "    #parcel = pd.DataFrame(df_parcel.drop(columns='geometry'))\n",
    "\n",
    "    # then merge using objectid\n",
    "    #merged_violation_data = parcel.merge(df_group, on='objectid', how='outer')\n",
    "    \n",
    "    # replace nan with 0\n",
    "    #merged_violation_data['violation_count'] = merged_violation_data['violation_count'].fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process building permit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_permit(df_parcel, df_permit, year):\n",
    "    \n",
    "    # in order to spatial join, only need id and geometry to keep it simple\n",
    "    df_parcel_simple = df_parcel[['objectid', 'geometry']]\n",
    "    \n",
    "    # select year \n",
    "    df_permit['Issued Date'] = pd.to_datetime(df_permit['Issued Date'])\n",
    "    df_permit['year'] = df_permit['Issued Date'].dt.year\n",
    "    df_permit = df_permit[(df_permit['year'] == year)]\n",
    "    \n",
    "    # this need to drop na in geometry fileds, otherwise it would fail\n",
    "    df_permit = df_permit.dropna(subset=['lat','long'])\n",
    "    gdf_permit = gpd.GeoDataFrame(df_permit, geometry=gpd.points_from_xy(df_permit.long, df_permit.lat))\n",
    "\n",
    "    # make sure the two dataset have the same projection\n",
    "    gdf_permit.crs = df_parcel_simple.crs\n",
    "    \n",
    "    permit_sjoin = gpd.sjoin(gdf_permit, df_parcel_simple, how=\"inner\", op='within')\n",
    "    \n",
    "    # dropnas\n",
    "    permit_sjoin = permit_sjoin.dropna()\n",
    "    \n",
    "    # group by parcel id\n",
    "    grouped = permit_sjoin.groupby('objectid').size()\n",
    "    df_group = grouped.to_frame().reset_index()\n",
    "    df_group.columns = ['objectid','building_permit']\n",
    "    \n",
    "\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assemble the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruoying\\anaconda3\\envs\\geoenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (10,12,13,20,38,45,46,55,59) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\Ruoying\\anaconda3\\envs\\geoenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (1,7,11,15,16,21,29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\Ruoying\\anaconda3\\envs\\geoenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (22,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "###################### Fire data\n",
    "parcel_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_parcel_lu/sf_parcel.geojson'\n",
    "df_parcel = gpd.read_file(parcel_file)\n",
    "\n",
    "fire_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/Fire_Incidents_SF.csv'\n",
    "df_fire = pd.read_csv(fire_file)\n",
    "\n",
    "# based on others\n",
    "structure_fire = ['111','112']\n",
    "\n",
    "df_structure_fire = df_fire[df_fire['Primary Situation'].str.contains('|'.join(structure_fire))]\n",
    "\n",
    "# create a df for internal fire.\n",
    "internal_fire = ['412','210','522','520','113','411','444','653','445','730',\\\n",
    "                 '220','223','114','410','440','652','221','521','650','422',\\\n",
    "                 '213','671','200','441','443','423','420','431','251','371',\\\n",
    "                 '212','222','211','231','115','424','430','440','117','411',\\\n",
    "                 '531','412','210','651','445','200','413','114','451','443',\\\n",
    "                 '163','116','231','431']\n",
    "df_internal_fire = df_fire[df_fire['Primary Situation'].str.contains('|'.join(internal_fire))]\n",
    "df_internal_fire.shape\n",
    "\n",
    "######################### inspection data\n",
    "inspection_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_fire_data/fire-inspections.csv'\n",
    "df_inspect = pd.read_csv(inspection_file)\n",
    "\n",
    "#  'Complaint Inspection'\n",
    "df_inspect = df_inspect[df_inspect['Inspection Type Description'] == 'Complaint Inspection']\n",
    "\n",
    "#################### violation data\n",
    "violation_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_fire_data/fire-violations.csv'\n",
    "df_violate = pd.read_csv(violation_file)\n",
    "\n",
    "################ permit data\n",
    "permit_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_building_permit/Building_Permits.csv'\n",
    "df_permit = pd.read_csv(permit_file)\n",
    "\n",
    "df_permit['Location'] = df_permit['Location'].str.strip('()')\n",
    "df_permit[['lat','long']] = df_permit['Location'].str.split(', ',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2008 2003 2004 2007 2005 2006 2009 2010 2011 2012 2013 2014 2015 2016\n",
      " 2017 2018 2019 2020]\n",
      "[2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007\n",
      " 2006 2005 2004]\n",
      "[2019 2018 2016 2014 2009 2020 2017 2008 2006 2011 2010 2007 2012 2015\n",
      " 2005 2013]\n"
     ]
    }
   ],
   "source": [
    "############################################# don't need to run this again\n",
    "# check the year of each dataset\n",
    "# seems that 2006 is the earliest for violation data\n",
    "df_fire['Incident Date'] = pd.to_datetime(df_fire['Incident Date'])\n",
    "df_fire['year'] = df_fire['Incident Date'].dt.year\n",
    "print(df_fire['year'].unique())\n",
    "\n",
    "df_inspect['Inspection Start Date'] = pd.to_datetime(df_inspect['Inspection Start Date'])\n",
    "df_inspect['year'] = df_inspect['Inspection Start Date'].dt.year\n",
    "print(df_inspect['year'].unique())\n",
    "\n",
    "df_violate['violation date'] = pd.to_datetime(df_violate['violation date'])\n",
    "df_violate['year'] = df_violate['violation date'].dt.year\n",
    "print(df_violate['year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# initiate df\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# so start from 2007, up to 2019\n",
    "# permit data only 2013-2018\n",
    "\n",
    "for year in range(2012,2020):\n",
    "    \n",
    "    building_fire = process_fire(df_parcel, df_internal_fire, year)\n",
    "\n",
    "    inspect_result = process_inspection(df_parcel, df_inspect, year)\n",
    "    violate_result = process_violation(df_parcel, df_violate, year)\n",
    "    permit_result = process_permit(df_parcel, df_permit, year)\n",
    "    \n",
    "    # merge inspection, violation data to building/fire\n",
    "    building_fire = building_fire.merge(inspect_result,on='objectid', how='outer')\n",
    "    building_fire = building_fire.merge(violate_result,on='objectid', how='outer')\n",
    "    building_fire = building_fire.merge(permit_result,on='objectid', how='outer')\n",
    "    \n",
    "    # replace nan with 0\n",
    "    building_fire['complaints_count'] = building_fire['complaints_count'].fillna(0)\n",
    "    building_fire['violation_count'] = building_fire['violation_count'].fillna(0)\n",
    "    building_fire['building_permit'] = building_fire['building_permit'].fillna(0)\n",
    "    \n",
    "    final_df = pd.concat([final_df,building_fire])\n",
    "    \n",
    "    print(year, end =\", \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1243744, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['yrbuilt', 'from_st', 'total_uses', 'shape_area', 'cie', 'landuse',\n",
       "       'mips', 'mapblklot', 'objectid', 'shape_leng', 'visitor', 'st_type',\n",
       "       'bldgsqft', 'resunits', 'pdr', 'street', 'retail', 'to_st', 'block_num',\n",
       "       'blklot', 'lot_num', 'med', 'fire_count', 'fire_count_last year',\n",
       "       'year', 'complaints_count', 'violation_count', 'building_permit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lastly: join parcel with block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/SF_parcel_lu/sf_parcel.geojson'\n",
    "df_parcel = gpd.read_file(parcel_file)\n",
    "\n",
    "block_file = 'D:/Study/insight/project/fire_risk_prediction_SF/data/Census 2010_ Blocks for San Francisco.geojson'\n",
    "df_block = gpd.read_file(block_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcel_simple = df_parcel[['objectid', 'geometry']]\n",
    "\n",
    "df_block['block_id'] = pd.to_numeric(df_block.geoid10, errors='coerce').astype(np.int64)\n",
    "df_block_simple = df_block[['block_id', 'geometry']]\n",
    "\n",
    "df_block_simple.crs = df_parcel_simple.crs\n",
    "\n",
    "parcel_sjoin = gpd.sjoin(df_parcel_simple, df_block_simple, how=\"inner\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectid</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>block_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((-122.42108 37.80478, -122.4211...</td>\n",
       "      <td>1999</td>\n",
       "      <td>60750102001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((-122.42118 37.80476, -122.4212...</td>\n",
       "      <td>1999</td>\n",
       "      <td>60750102001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((-122.42126 37.80475, -122.4213...</td>\n",
       "      <td>1999</td>\n",
       "      <td>60750102001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((-122.42135 37.80474, -122.4215...</td>\n",
       "      <td>1999</td>\n",
       "      <td>60750102001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((-122.42153 37.80472, -122.4217...</td>\n",
       "      <td>1999</td>\n",
       "      <td>60750102001011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  objectid                                           geometry  index_right  \\\n",
       "0        1  MULTIPOLYGON (((-122.42108 37.80478, -122.4211...         1999   \n",
       "1        2  MULTIPOLYGON (((-122.42118 37.80476, -122.4212...         1999   \n",
       "2        3  MULTIPOLYGON (((-122.42126 37.80475, -122.4213...         1999   \n",
       "3        4  MULTIPOLYGON (((-122.42135 37.80474, -122.4215...         1999   \n",
       "4        5  MULTIPOLYGON (((-122.42153 37.80472, -122.4217...         1999   \n",
       "\n",
       "         block_id  \n",
       "0  60750102001011  \n",
       "1  60750102001011  \n",
       "2  60750102001011  \n",
       "3  60750102001011  \n",
       "4  60750102001011  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcel_sjoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158633, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcel_sjoin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_sjoin = parcel_sjoin[['objectid','block_id']]\n",
    "parcel_sjoin.to_csv('parcel_blk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "geoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
